# SmartMed
## DGLNet: A cross-modal global-local collaborative network for chest radiology report generation
Automatic chest radiology report generation is crucial for improving diagnostic efficiency, enhancing diagnostic quality, and reducing misdiagnoses or missed diagnoses. Existing approaches either resize the high-resolution radiology image into smaller sizes or segment it into multiple patches for separate processing, which may lose fine local details or global contextual information, and the accuracy of report generation will be limited. In addition, the direct fusion of visual features with sentence features reduces the accuracy of aligning fine-grained visual features with text features. In this paper, we focus on designing a cross-modal global-local collaborative network to capture more local fine details and global contextual information, which comprises a global-to-local network and a local-to-global network that takes the resized entire image and segmented image patches as respective inputs. The former handles the extraction of global visual features, the fusion of global and local visual features, and the fusion of visual features into report sentences. The latter implements the extraction of local visual features, the fusion of local to global visual features, and the fusion of visual features to report words. Experimental results on two publicly available datasets, IU X-Ray and MIMIC-CXR, show that our proposed network outperforms state-of-the-art image captioning-based methods in most evaluation metrics.
## Requirements
* Python >= 3.6
* Pytorch >= 1.7
* torchvison
# Dataset
We evaluate the proposed model on two typical publicly available chest radiology datasets, IU X-Ray and MIMIC-CXR, that are partitioned into the train, validation, and test datasets by 6:2:2.IU X-Ray is a public radiology image and report dataset and is broadly used to evaluate the performance in previous works of chest radiology report generation, which includes 7470 X-ray images associated with 3955 radiology reports. We get 7412 image-report pairs by removing the data that impressions or findings are null, and one free-text report may correspond to multiple images. Similar to the previous work, we preprocess each report by converting all tokens to lowercase and filtering the tokens with no more than three occurrences, resulting in 835 unique words.MIMIC-CXR is a large-scale labeled dataset of 377,110 chest x-ray images associated with 227,827 radiology reports. Each free-text report corresponds to one or more orientation images and pathology labels. We screen 29521 one-to-one image-report pairs with complete impressions and findings from the MIMIC-CXR dataset because we only focus on considering the bidirectional fusion of global and local visual features of the image without considering the fusion between the multi-view features. Similar to the previous work, we preprocess each report by converting all tokens to lowercase and filtering the tokens with no more than three occurrences, resulting in 4182 unique words.
* IU dataset from [here](https://openi.nlm.nih.gov/)
* MIMIC-CXR dataset from [here](https://www.physionet.org/content/mimic-cxr/2.0.0/)
